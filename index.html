<!DOCTYPE html>
<html lang="en">

<head>
  <title>Multi-Modal Pre-Training Workshop</title>
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8" />
  <link rel="stylesheet" href="/static/css/bootstrap.css">
  <link rel="stylesheet" type="text/css" href="/static/css/prettify.css">
  <script src="/static/js/jquery.min.js"></script>
  <script src="/static/js/bootstrap.js"></script>
  <script src="/static/js/sprintf.js"></script>
  <script src="/static/js/run_prettify.js"></script>
  <script src="/static/js/md5.js"></script>
  <style type="text/css">
    body { 
      scroll-behavior: smooth;
    }
    #nav-first {
      border-radius: 0px;
      margin-bottom: 0px;
      min-height: 80px;
    }

    #nav-second {
      border-radius: 0px;
      margin-bottom: 0px;
      top: 80px;
    }

    #nav-first-main {
      margin: 0 0;
      height: 80px;
    }

    #nav-first-main>li {
      height: 80px;
    }

    #nav-first-main>li>a {
      height: 80px;
      padding-top: 0;
      padding-bottom: 0;
      line-height: 80px;
    }

    #nav-second-main {
      margin: 0 80px;
    }

    .nav-second-li {
      font-size: 120%;
    }


    #jumbotron {
      height: 600px;
      background-image: url(/static/img/background.jpg);
      background-size: cover;
      margin-bottom: 0px;
      padding: 0;
      margin-top: -20px;
    }

    #block1 {
      background-color: white;
    }

    #block2 {
      /*background-color: #f8f8f8;*/
      background-color: #e8e8e8;
    }

    #block3 {
      background-color: white;
    }

    #block4 {
      background-color: #e8e8e8;
    }

    #block5 {
      background-color: white;
    }

    #block6 {
      background-color: #e8e8e8;
    }

    #block7 {
      background-color: white;
    }

    #block8 {
      background-color: #e8e8e8;
    }

    .p-body {
      text-align: justify;
      font-family: "Arial", sans-serif;
      font-size: 120%;
    }

    .p-body-small {
      text-align: justify;
      font-family: "Arial", sans-serif;
      font-size: 80%;
    }

    h1,
    h2 {
      font-family: "Arial", serif;
      /*font-size: 150%;*/
    }

    .timeline-li {
      font-family: "Arial", sans-serif;
      font-size: 120%;
    }

    #logo {
      padding-left: 100px;
      padding-top: 10px;
    }

    .thumbnail {
      border-color: transparent;
      background-color: transparent;
    }

    .avatar {
      border-radius: 50%;
    }

    .nav-li {
      font-size: 120%;
      cursor: pointer;
    }

    .dropdown-li {
      background-color: #333333;
    }
    .dropdown-li>a {
      color: #aaaaaa !important;
    }
    .dropdown-menu {
      background-color: #333333;
    }

    td,
    th {
      text-align: center;
    }

    .mengban {
      width: 100%;
      height: 100%;
      background-color: rgba(0, 0, 0, 0.5);
    }

    #morning {
      /*color: rgb(248,206,69);*/
      padding-top: 200px;
    }

    .orange {
      color: rgb(241, 156, 55) !important;
    }

    .white {
      color: white !important;
    }

    .red {
      color: red !important;
    }

    .help-block {
      display: none;
    }

    .member-title {
      padding-top: 10px;
    }

    .p-title {
      color: rgb(241, 156, 55) !important;
    }

    .p-title-2 {
      color: rgb(123, 123, 123) !important;
    }


    .timeline {
      display: block;
      margin: 15px 0
    }

    .timeline ul {
      margin-left: 25px;
      margin-top: 30px;
      border-left: 2px solid #ddd;
      list-style: none;
    }

    .timeline ul li {
      width: 100%;
      margin-left: -51px;
      line-height: 20px;
      margin-bottom: 20px
    }

    .timeline ul li b {
      width: 10px;
      height: 10px;
      background: #fff;
      border: 2px solid rgb(241, 156, 55);
      margin: 0 5px;
      border-radius: 6px;
      -webkit-border-radius: 10px;
      -moz-border-radius: 10px;
      overflow: hidden;
      display: inline-block;
      float: left
    }

    .timeline ul li span {
      padding-left: 7px;
      font-size: 16px;
      line-height: 20px;
      color: rgb(241, 156, 55);
      font-weight: bold
    }

    .timeline ul li p {
      padding-left: 25px;
      font-size: 13px;
      line-height: 25px;
      color: #666
    }

    .timeline ul li h4 {
      padding-left: 25px;
    }

    .timeline ul li h5 {
      padding-left: 25px;
    }

    .timeline ul li img {
      padding-left: 25px;
    }

    #forget {
      cursor: pointer;
    }

    .name {
      height: 38px;
    }

    .organization {
      height: 40px;
    }
  </style>
</head>

<body>


  <div class="jumbotron" id="jumbotron">
    <div class="mengban">
      <center>
	      <h1 id="morning">
		      <span class="orange">M</span><span class="white">ulti-</span><span class="orange">M</span><span class="white">odal </span><span class="orange">P</span><span class="white">re-</span><span class="orange">T</span><span class="white">raining for Multimedia Understanding</span>
        </h1>
      </center>
      <center>
        <h2 class="white">ACM International Conference on Multimedia Retrieval (ICMR) Workshop</h2>
      </center>
      <center>
        <h3 class="white">16 - 19 November <b class="orange">2021</b>, Taipei, Taiwan, China</h3>
      </center>
    </div>
  </div>

  <nav class="navbar navbar-inverse">
    <div class="container">
      <ul class="nav navbar-nav">
        <li class="nav-li"><a class="nav-second-li" href="#block1"><b>Introduction</b></a></li>
        <li class="nav-li"><a class="nav-second-li" href="#block3"><b>Date</b></a></li>
        <li class="nav-li"><a class="nav-second-li" href="#block4"><b>Submission</b></a></li>
        <li class="nav-li"><a class="nav-second-li" href="#block5"><b>Camera-ready Instruction</b></a></li>
        <li class="nav-li"><a class="nav-second-li" href="#block6"><b>Program</b></a></li>
        <li class="nav-li"><a class="nav-second-li" href="#block7"><b>Organizers</b></a></li>
        <li class="nav-li"><a class="nav-second-li" href="#block8"><b>Contact</b></a></li>
      </ul>
    </div>
  </nav>


    <div class="row" id="block1" name="introduction">
      <div class="container">
        <div class="col-xs-12">
          <h2 class="p-title"><b>Introduction</b></h2>
          <p class="p-body">
            Representation learning has always been a key challenge in many tasks of multi-modality domain, such as image-text retrieval, visual question and answering, video localization, speech recognition, ect. Along the history of multi-modality learning, we find that model initialization is one of the most important factors. For example, research of weight initialization set a foundation of neural network based methods and image features pre-trained with Visual Genome Dataset build a new standard setting for many vision-language models. Recently, multi-modal pre-training is a new paradigm of model initialization that establishes the state-of-the-art performances for many multimedia tasks. Pretraining models outperforms traditional methods by providing stronger representation of different modalities learned in an unsupervised training way. Multi-modal pre-training is an interesting topic and has attracted rapidly growing interests in many fields and the intersection of these them, including computing vision, natural language processing, speech recognition, etc. With the continuous effort of many works, we also find that the cost time can be even decreased to 10 hours on 2 Titan RTXs for a vision-language pre-training model in very recent works. Although the emerging trend of multi-modal pre-training models, it remains unexplored in many aspects. For example, studies of standard settings for fair comparison of different multi-modal pre-training models will benefit the research community. More discussion about the efficiency of sub-modules and pre-training tasks will also help us to have more thorough knowledge about pre-training mechanism. Exploration of improving training efficiency is also worth tackling.
          </p>
          <br>
        </div>
      </div>
    </div>



    <div class="row" id="block2" name="submission">
      <div class="container">
        <div class="col-xs-12">
          <h2 class="p-title"><b>Call For Paper</b></h2>
          <p class="p-body">
            The goals of this workshop are to (1) investigate research opportunities of multi-modal model initialization, especially on multi-modal pre-training, (2) solicit novel methodologies of multi-modal pre-training, (3) explore and discuss the advantage and possibilities of pre-training for more multimedia tasks. We expect contributions concerning multimodality model initialization and multi-modal pre-training, involving image, language, video, speech, etc.
	  </p>
	  <p class="p-body">
	    The topics of interest include (but not limited to):
	    <ul>
              <li>Multi-modal self-supervised learning</li>
              <li>Multi-modal pre-training task</li>
              <li>Multi-modal pre-training optimization</li>
              <li>Multi-modal representation learning</li>
              <li>Multi-modal model optimization</li>
              <li>Multi-modal model initialization</li>
              <li>Cross-modality retrieval</li>
              <li>Lightweight multi-modal pre-training</li>
              <li>Multi-modality alignment and parsing</li>
              <li>Advanced multi-modal applications</li>
              <li>Benchmark datasets and novel evaluation methods</li>
            </ul>
          </p>
          <br>
        </div>
      </div>
    </div>

    <div class="row" id="block3">
      <div class="container">
        <div class="col-xs-12">
          <h2 class="p-title"><b>Important Dates</b></h2>
          <!--p class="p-body-small">As you know, MM’20 has extended some deadlines to accommodate the impact caused by COVID-19. We confirmed with general chairs accordingly. As grand challenge papers will be included in the main proceedings, we need to follow the camera-ready deadline for full papers, which is updated to 8/10/2020 (<a href="https://2020.acmmm.org/call-for-paper.html">https://2020.acmmm.org/call-for-paper.html</a>). Please check the new deadlines as follows.
          </p-->
          <div class="timeline">
            <ul>
	      <li><b></b><span><s>April 20, 2021</s>  April 25, 2021</span>
                <p>Deadline for Workshop Paper Submission.</p>
              </li>
	      <li><b></b><span><s>May 20, 2021</s>  May 15, 2021</span>
                <p>Acceptance Notification of Workshop Papers.</p>
              </li>
		    <li><b style="border: 2px solid rgb(255, 0, 0)"></b><span class="red"><s> May 30, 2021</s> June 20, 2021</span>
		 <p>Camera-ready date for Workshop Papers.</p>
              </li>  
            </ul>
          </div>
          <br>
        </div>
      </div>
    </div>

    <div class="row" id="block4" name="submission">
      <div class="container">
        <div class="col-xs-12">
          <h2 class="p-title"><b>Paper Submission</b></h2>
	  <h3 class="p-title-2"><b>Paper Format</b></h3>
          <p class="p-body">
	  All papers must be formatted according to the ACM proceedings style. <a href="https://www.acm.org/publications/proceedings-template">Click on the link</a> to access Latex and Word templates for this format. Please use "sample-sigconf.tex" as a Latex template or "ACM_SigConf.doc" as a Word template.
	  </p>
	  <br>
	  <h3 class="p-title-2"><b>Lenght Of The Paper</b></h3>
          <p class="p-body">
We invite the following two types of papers:
<br><br>
<b>Full Paper:</b> limited to 8 pages, including all text, figures, and references: Full Papers should describe original contents with evaluations. They will be reviewed by more than two experts based on:
<br>
&nbsp;&nbsp;1. Originality of the content
<br>
&nbsp;&nbsp;2. Quality of the content based on evaluation
<br>
&nbsp;&nbsp;3. Relevance to the theme
<br>
&nbsp;&nbsp;4. Clarity of the written presentation
<br><br>
<b>Short Paper:</b> limited to 4 pages, including all text, figures, and references. Short papers should describe work in-progress as position papers. They will be reviewed by two experts based on:
<br>
&nbsp;&nbsp;1. Originality of the content
<br>
&nbsp;&nbsp;2. Relevance to the theme
<br>
&nbsp;&nbsp;3. Clarity of the written presentation
          </p>
	  <h3 class="p-title-2"><b>Submission Website</b></h3>
          <p class="p-body">
Submissions should be made through <a href="https://easychair.org/conferences/?conf=acmicmr2021">here</a>.
          </p>
          <br>
        </div>
      </div>
    </div>

    <div class="row" id="block5" name="program">
      <div class="container">
        <div class="col-xs-12">
          <h2 class="p-title"><b>Camera-ready Instruction</b></h2>
          <p class="p-body">
	  The preparation instructions are readied for authors' final submissions. Please review: <a href="http://www.scomminc.com/pp/acmsig/mmpt.htm">http://www.scomminc.com/pp/acmsig/mmpt.htm</a> (for authors &amp; speakers' submission types and specific deadlines).</p>
	  <p class="p-body">The MMPT'21 recorded presentation video instructions: <a href="http://www.scomminc.com/pp/acmsig/MMPT-present-video.htm">www.scomminc.com/pp/acmsig/MMPT-present-video.htm</a>
	  </p>
          <br>
        </div>
      </div>
    </div>



    <div class="row" id="block6" name="program">
      <div class="container">
        <div class="col-xs-12">
          <h2 class="p-title"><b>Program</b></h2>

          <div class="timeline">
            <ul>
              <li>
                <b></b>
                <span>9:00-9:05</span>
                <h4>Opening</h4>
              </li>
        
              <li>
                <b></b>
                <span>9:05-9:40</span>

                
                <div class="row">
                  <div class="col-xs-2">
                    <img src="/static/img/ruihua_song.jpg" width="100%">
                  </div>
                  <div class="col-xs-10" style="margin-left: -10px">
                    <h3>&nbsp;&nbsp;&nbsp;&nbsp;Ruihua Song</h3>
                    <p>Dr. Ruihua SONG is a tenured Associate Professor of Gaoling School of AI at Renmin University. She has been Lead Researcher of Microsoft Research Asia and Chief Scientist of Microsoft XiaoIce. On May, 2017, the first AI-authored poem collection was published. The book title is “The Sunshine Lost Windows”. She contributes the generation algorithm behind. She published more than 90 papers and hold 25 patents. She serves SIGIR, ACL, KDD, WWW, EMNLP, CIKM, etc. as Short Paper Chairs, Area Chair, Senior PC, or PC. Her recent research interests include multi-modal understanding of natural language, multi-modal dialogue systems and AI based creation.</p>
                  </div>
                </div>
                  

                <h4> Title: <strong>WenLan: Efficient Large-Scale Multi-Modal Pre-Training on Real World Data</strong></h4>
		      <p> <strong>Abstract: </strong><br>Multi-modal pre-training models have been intensively explored to bridge vision and language in recent years. However, most of them explicitly model the cross-modal interaction between image-text pairs, by assuming that there exists strong semantic correlation between the text and image modalities. Since this strong assumption is often invalid in real-world scenarios, we choose to implicitly model the cross-modal correlation for large-scale multi-modal pre-training, which is the focus of the Chinese project `WenLan' led by our team. Specifically, with the weak correlation assumption over image-text pairs, we propose a two-tower pre-training model called BriVL within the cross-modal contrastive learning framework. We construct a large Chinese multi-source dataset of 650 million image-text pairs for pre-training our model. Extensive experiments demonstrate that WenLan on various downstream tasks and easy to build efficient applications based on searching between images and texts.</p>

              </li>
        
              <li>
                <b></b>
                <span>9:45-10:30</span>
                <h4>Session 1</h4>
                <p>&nbsp;&nbsp; 9:45-10:00: &nbsp;&nbsp; Be Specific, Be Clear: Bridging Machine and Human Captions by Scene-Guided Transformer</p>
                <p>&nbsp;&nbsp; 10:00-10:15: &nbsp;&nbsp;Language-Conditioned Region Proposal and Retrieval Network for Referring Expression Comprehension</p>
                <p>&nbsp;&nbsp; 10:15-10:30: &nbsp;&nbsp;Residual Recurrent CRNN for End-to-End Optical Music Recognition on Monophonic Scores</p>
              </li>
              
              <li>
                <b></b>
                <span>10:35-11:10</span>
                <div class="row">
                  <div class="col-xs-2">
                    <img src="/static/img/limin_wang.jpg" width="100%">
                  </div>
                  <div class="col-xs-10" style="margin-left: -10px">
                    <h3>&nbsp;&nbsp;&nbsp;&nbsp;Limin Wang</h3>
                    <p>Dr. Limin Wang received the B.Sc. degree from Nanjing University, Nanjing, China, in 2011, and the Ph.D. degree from the Chinese University of Hong Kong, Hong Kong, in 2015. From 2015 to 2018, he was a Post-Doctoral Researcher with the Computer Vision Laboratory, ETH Zurich. He is currently a Professor with the Department of Computer Science and Technology, Nanjing University. His research interests include computer vision and deep learning. He was the first runner-up at the ImageNet Large Scale Visual Recognition Challenge 2015 in scene recognition, and the winner at the ActivityNet Large Scale Activity Recognition Challenge 2016 in video classification. He has served as a Senior PC or Area Chair for AAAI 2021, IJCAI 2021. He is a member of the IEEE and ACM.</p>
                  </div>
                </div>
                <h4> Title: <strong>Cross-modal Pretraining and Matching for Video Understanding</strong></h4>
                <p> <strong>Abstract: </strong> <br>Videos are generally accompanied with multi-modal information such as audio, text, and motion. The multi-modal information is becoming an important cue for understanding video content.  How to model the correlation between multi-modalities in videos is still an unsolved problem in video understanding tasks such as video action recognition, video temporal grounding, and video description.

In this talk, we focus on two specific video understanding tasks (i.e., cross-modal self-supervised pretraining and temporal grounding) by exploiting the video-text cross modal information. In particular, we notice that videos are naturally accompanied by abundant text information such as YouTube titles, Instagram captions, and Movie scripts. This textual information could serve as a general information to guide us train a multi-modal network, which could be used as a general video representation to be fine-tuned on the downstream tasks, or as cross-modal matching similarity to be used for video segment retrieval.
</p>
              </li>

              <li>
                <b></b>
                <span>11:10-11:55</span>
                <h4>Session 2</h4>
                <p>&nbsp;&nbsp; 11:10-11:25: &nbsp;&nbsp;Style-Guided Image-to-Image Translation for Multiple Domains</p>
                <p>&nbsp;&nbsp; 11:25-11:40: &nbsp;&nbsp;A Fair and Comprehensive Comparison of Multimodal Tweet Sentiment Analysis Methods</p>
                <p>&nbsp;&nbsp; 11:40-11:55: &nbsp;&nbsp;Unsupervised Training Data Generation of Handwritten Formulas using Generative Adversarial Networks with Self-Attention</p>
              </li>

              <li>
                <b></b>
                <span>11:55-12:00</span>
                <h4>Closing</h4>
              </li>

            </ul>
          </div>

          <br>
        </div>
      </div>
    </div>

    <div class="row" id="block7" name="organizers">
      <div class="container">
        <div class="col-xs-12">
          <h2 class="p-title"><b>Organizers</b></h2>
          <br>
          <div class="row">
            <div class="col-xs-2">
              <div class="thumbnail">
                <img src="/static/img/Bei_Liu.jpg" width="100%" class="avatar">
                <div class="caption">
                  <center>
                    <h4 class="name">Bei Liu</h4>
                  </center>
                  <p>
                    <center class="organization">Microsoft Research Asia</center><br>
                  </p>
                  <p>
                    <center><a href="mailto:bei.liu@microsoft.com">bei.liu@microsoft.com</a></center>
                  </p>
                </div>
              </div>
            </div>

            <div class="col-xs-2">
              <div class="thumbnail">
                <img src="/static/img/JianlongFu.jpeg" width="100%" class="avatar">
                <div class="caption">
                  <center>
                    <h4 class="name">Jianlong Fu</h4>
                  </center>
                  <p>
                    <center class="organization">Microsoft Research Asia</center><br>
                  </p>
                  <p>
                    <center><a href="mailto:jianf@microsoft.com">jianf@microsoft.com</a></center>
                  </p>
                </div>
              </div>
            </div>

            <div class="col-xs-2">
              <div class="thumbnail">
                <img src="/static/img/Shizhe_Chen.jpg" width="100%" class="avatar">
                <div class="caption">
                  <center>
                    <h4 class="name">Shizhe Chen</h4>
                  </center>
                  <p>
                    <center class="organization">INRIA</center><br>
                  </p>
                  <p>
                    <center><a href="mailto:shizhe.chen@inria.fr">shizhe.chen@inria.fr</a></center>
                  </p>
                </div>
              </div>
            </div>

            <div class="col-xs-2">
              <div class="thumbnail">
                <img src="/static/img/Qin_Jin.jpg" width="100%" class="avatar">
                <div class="caption">
                  <center>
                    <h4 class="name">Qin Jin</h4>
                  </center>
                  <p>
                    <center class="organization">Renmin University of China</center><br>
                  </p>
                  <p>
                    <center><a href="maito:qjin@ruc.edu.cn">qjin@ruc.edu.cn</a></center>
                  </p>
                </div>
              </div>
            </div>
            <div class="col-xs-2">
              <div class="thumbnail">
                <img src="/static/img/Hauptmann_Alexander.jpg" width="100%" class="avatar">
                <div class="caption">
                  <center>
                    <h4 class="name">Alexander Hauptmann</h4>
                  </center>
                  <p>
                    <center class="organization">Carnegie Mellon University</center><br>
                  </p>
                  <p>
                    <center><a href="maito:alex@cs.cmu.edu">alex@cs.cmu.edu</a></center>
                  </p>
                </div>
              </div>
            </div>

            <div class="col-xs-2">
              <div class="thumbnail">
                <img src="/static/img/Yong_Rui.jpg" width="100%" class="avatar">
                <div class="caption">
                  <center>
                    <h4 class="name">Yong Rui</h4>
                  </center>
                  <p>
                    <center class="organization">Lenovo Group</center><br>
                  </p>
                  <p>
                    <center><a href="maito:yongrui@lenovo.com">yongrui@lenovo.com</a></center>
                  </p>
                </div>
              </div>
            </div>

	  </div>

        </div>
      </div>
    </div>

    <div class="row" id="block8" name="contact">
      <div class="container">
        <div class="col-xs-12">
          <h2 class="p-title"><b>Contact</b></h2>
          <p class="p-body">
	  <a href="mailto:multimodal.pretraining@gmail.com">multimodal.pretraining@gmail.com</a>
	  </p>
          <br>
        </div>
      </div>
    </div>


  </div>

</body>

</html>
